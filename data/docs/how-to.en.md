# How to use the app

Are you a teacher, a student, or just a curious passerby?

Welcome! In this short overview, you will learn what this app can do and how to make the most of it.

## What is this app for?

**Letâ€™s start with what AnimatedLLM is not**.

AnimatedLLM isnâ€™t (at least for now!) a comprehensive educational app that will guide you through the inner workings of large language models from start to finish.

To fully understand the topic, you will need to look into other materials as well.

For a start, try this short video from the [3Blue1Brown](https://www.youtube.com/@3blue1brown/videos) channel (English, with subtitles in many languages):

<iframe width="560" height="315" src="https://www.youtube.com/embed/LPZh9BOjkQs?si=Nogb8BgfP9kL1DIa" title="YouTube video player" frameborder="0" allow="clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

You can find many other materials online. High-quality resources include these illustrated blog posts by Jay Alammar:

- **[Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/)**
- **[Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)**

For Czech students, there are university courses directly focused on large language models:

- **MFF UK:** [Large Language Models (NPFL140)](https://ufal.mff.cuni.cz/courses/npfl140)
- **FIT ÄŒVUT:** [Neural Language Models (NI-NLM)](https://bilakniha.cvut.cz/en/predmet8241006.html)
 
 
(Disclaimer: the author of this app contributes to the first course and leads the second one ðŸ˜‡)

**So, what is AnimatedLLM for?**

The animations in AnimatedLLM serve primarily as a teaching aid. They help explain the principles of how language models work, which would otherwise require a lot of "hand-waving" or drawing on a whiteboard.

For example:
- Where does the text generated by the model come from?
- What does it mean for a model to be "trained on documents"?
- Where exactly do the next-token probabilities come from?
- etc.


However, why we address these processes in the first place, how language models originated, and what their role is in complex services like ChatGPT, Gemini, or Claudeâ€”that's something you'll need to study elsewhere.


**One more warning...**

Some principles of language models are simplified in AnimatedLLM. Not too much, but some technical details are omitted for the sake of clarity.

If you are interested in these specific technical details, the following animations go much deeper into model architecture:
- [llm-viz](https://bbycroft.net/llm)
- [Transformer Explainer](https://poloclub.github.io/transformer-explainer/)

## How does the app work?

The animations in AnimatedLLM are **recorded interactions with real language models**.

This means two things:
- The animations show the realistic behavior of actual language models.
- No language model is actually running during the animations; it's just a recording playing in your browser.

### How were the recordings created?
The recordings were generated using smaller open-source language models from the [Hugging Face](https://huggingface.co/models) repository.

The models ran on a university computing cluster using the [vLLM](https://github.com/vllm-project/vllm) framework. This framework allows for the collection of fine-grained details, such as next-token probabilities.

Prompts and documents were chosen to demonstrate various interesting phenomena (e.g., "why can't models count letters very well?").

Data is available for different:
- models,
- decoding temperatures,
- languages.

You can find the scripts for generating the recordings in the [project repository](https://github.com/kasnerz/animated-llm/tree/main/scripts/data-generation).

There is also a script you can use to [download and explore](https://github.com/kasnerz/animated-llm/blob/main/scripts/download_data.py) all current recordings in JSON format.

### What data is in the recordings?

âš  Note: This information may have partially changed since it was written.

#### Model
The app includes outputs from the following models:
- [CohereForAI/aya-expanse-8b](https://huggingface.co/CohereForAI/aya-expanse-8b)
- [meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)
- [Qwen/Qwen3-4B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507)
- [allenai/Olmo-3-7B-Think](https://huggingface.co/allenai/Olmo-3-7B-Think)
- [openai-community/gpt2-xl](https://huggingface.co/openai-community/gpt2-xl)

For training purposes, a recording from a "vanilla Transformer" is also availableâ€”essentially a model with randomly initialized weights (it's based on the `Llama-3.2-1B-Instruct` architecture, but that's just a technical detail).

#### Decoding temperature
The decoding temperature is a number between 0 and âˆž that (simply put) determines how randomly the next tokens are selected.

While a temperature near zero means the most likely token is almost always chosen, at a very high temperature, the model's predictions are barely considered, and tokens are chosen randomly.

Recordings are available in the app for the following temperature values:
- ðŸ§Š **0.0**: the most probable token is always selected,
- ðŸŒ¡ **1.0**: the token is selected randomly based on the model's probabilities,
- ðŸ”¥ **5.0**: the token is selected randomly, and the model's probabilities play only a minor role.

#### Language

We provide recordings in all languages supported by the app. Currently, these are:

- ðŸ‡¬ðŸ‡§ English
- ðŸ‡¨ðŸ‡¿ Czech
- ðŸ‡«ðŸ‡· French
- ðŸ‡ºðŸ‡¦ Ukrainian
- ðŸ‡¨ðŸ‡³ Chinese

The languages were chosen to cover a wide range of language families and scripts.

Interested in adding more languages or fixing errors? Jump down to the "How can I give feedback?" section.

âš  Note: Except for English and Czech, the website translations were created automatically using large language models.


## How to control the app
### How do I choose a specific recording?

1. On the home screen, select which aspect of language models you are interested in.
2. Choose a **model and temperature** in the settings. In some animations, these settings are visible directly; in others, they are hidden under the icon:

![settings](img/settings.png)

3. Choose a **language** via the flag icon. (Currently, changing the recording language is tied to switching the language of the entire app):

![language](img/languages.png)

4. Start the animation by clicking the "Play" button.

### Pro tip

The animations can also be controlled using keyboard shortcuts.

Instead of the "Play" button, you can step through the animation using the right arrow key, or go back a step using the left arrow key.

And there are many more shortcuts!

You can find all of them under the keyboard icon:

![keyboard](img/keyboard.png)


## How can I give feedback?

Please send questions and ideas for improvement to the **[GitHub discussion forum](https://github.com/kasnerz/animated-llm/discussions)**.

If you don't have a GitHub account, you can also write directly to the main author of the app ([ZdenÄ›k Kasner](https://kasnerz.github.io); you can find his email [here](https://ufal.mff.cuni.cz/zdenek-kasner)).